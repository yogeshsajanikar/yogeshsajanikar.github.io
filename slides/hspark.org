#+STARTUP: hidestars overview
#+TITLE: hspark
#+AUTHOR: Yogesh Sajanikar
#+EMAIL: 
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+REVEAL_THEME: night
#+OPTIONS: num:nil
#+OPTIONS: toc:nil

* Overview

  + [[http://spark.apache.org/][Apache Spark]] inspired
    - Distributed, in-memory computation 
    - Simple DSL to create RDD (Resilient Distributed Data)
      #+begin_src scala
        // Scala Code
        val sc = SparkContext ()
        val lines : RDD = sc.textFile("data.txt")
        val lineLengths : RDD = lines.map(s => s.length)
      #+end_src
    - Simple Map-Reduce over networked nodes

* Implementation

  + Based on cloud haskell
  + Uses _Static Pointers_ and _Remote Table_ (for polymorphic types)
  + Uses _Closure_ to spawn process on remote node(s)

* Stages
 
  - Data is distributed into /Blocks/. /Blocks/ reside on multiple /Nodes/
  - A /Block/ is a process that _holds_ the data, till it is asked by dependent /Block/
  - Mapping operations _typically_ evaluate on the same /Node/ 
  - Reduce causes shuffling
 
  #+begin_src dot :file rdd.svg
    digraph rdd {
            rankdir = LR
            ranksep=0.2
            node [ shape = rectangle ]
            start [ rank = "source" ]
            subgraph cluster_1 {
                    rankdir = LR
                    d1 [ label = "partition 1", rank = 1 ]
                    d2 [ label = "partition 2" ]
                    m1 [ label = "map 1" ]
                    m2 [ label = "map 2" ]
                    r1 [ label = "reduce 1" ]
                    label = "node 1"
            }
            subgraph cluster_2 {
                    rankdir = LR
                    d3 [ label = "partition 3" ]
                    d4 [ label = "partition 4" ]
                    m3 [ label = "map 3" ]
                    m4 [ label = "map 4" ]
                    r3 [ label = "reduce 3" ]
                    r4 [ label = "reduce 4" ]
                    label = "node 2"
            }

            start -> d1 [ label = "Distribute" ]
            start -> d2
            start -> d3
            start -> d4

            d1 -> m1 [label = "map f" ]
            d2 -> m2
            d3 -> m3
            d4 -> m4

            m1 -> r1; m1 -> r3
            m2 -> r1; m2 -> r4
            m3 -> r3
            m4 -> r1; m4 -> r4
            
            r1 -> end
            r3 -> end
            r4 -> end

            end [ label = "collect" ]
    }
  #+end_src

  #+RESULTS:
  [[file:rdd.svg]]

* RDD - Distributed Data
  + Context gives configuration to run the computation with
    #+begin_src haskell
      data Strategy = Distributed { masterNode :: NodeId, slaveNodes :: [NodeId] }
      data Context  = Context { _lookupTable :: RemoteTable -- Lookup table
                              , _strategy :: Strategy }

    #+end_src
  + Implemented as a set of *Processes* returning set of /Block/. Each block reprsenting chunk of data residing in a process.

    #+begin_src haskell
      class Serializable b => RDD a b where
          -- | Evaluate RDD and return the set of processes representing data 
          flow :: Context -> a b -> Process (Blocks b)
    #+end_src

* DSL - Sample 

  #+begin_src haskell
    sc <- createContextFrom remoteTable master slaves
    -- Create RDD with 2 partitions
    let partitions = Just 2
        dt = [1..10]
        -- Seed the data with 
        seed = seedRDD sc partitions dict ($(mkClosure 'input) dt)
        -- Map the data
        maps = mapRDD sc seed dict square
        -- Reduce with a combiner
        reduce = reduceRDD sc maps odict dict combiner partitioner

    -- Compute, will trigger seed, maps, reduce 
    result <- collect sc reduce

  #+end_src

  /Note: Dictionaries need to be passed for passing qualified type dictionaries/
  a.k.a. "Dict Trick"

* Execution 

  + Equal distribution among nodes
  + Maps are always localized (Run on the same node where parent process was executed).
  + Execution
    #+begin_src haskell
      instance (RDD a b, Serializable c) => RDD (MapRDD a b) c where

          flow sc (MapRDD base cfun tdict) = do
            -- Get the process IDs of the base process
            (Blocks pmap) <- flow sc base

            -- For each process, try to spawn process on the same node doing mapping
            mpids <- forM (M.toList pmap) $ \(i, pid) -> do
                        (Just pi) <- getProcessInfo pid
                        spawn (infoNode pi) (rddMapClosure (rddDictS base) tdict (i, pid)  cfun )
                        
            return $ Blocks $ M.fromList (zip [0..] mpids)
    #+end_src



* Process as computation and storage

  + *hspark* spawns process on remote nodes using closure
  + *hspark* treats process as /computation/ and /storage/
  + Each process carries out
    - _Fetching_ - Fetching data from parent process (may locate on same or other machine). 
    - _Computation_ - Doing actual work.
    - _Delivery_ - Delivering the computed data to child node.



* Reduction Step - Shuffling

  + Two step process
  + Step 1: 
    + Reduce locally
    + Hold the data for step 2
  + Step 2:
    + Send the partion number to Step 1 processes.
    + Step 1 process partition localization and deliver subset of data.
    + Partitioning function should ensure that it achieves independent partitions.
    + Locally do the reduction step 


* Limitations and Future Directions
  + Though tests work correctly, those needs to be enhanced
  + Controlling *life* of a process, proliferation of exceptions, and linking up processes.
  + Avoiding serialization of local node.
  + Adding backends for holding shuffled data (Hadoop, Mesos etc.)

* References

  + Apache Spark - Original Research Paper from Berkley University
    - https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf
  + Mapreduce commentry by Ralf Lammel
    - http://userpages.uni-koblenz.de/~laemmel/MapReduce/paper.pdf
  + Distributed Process (Hackage Documentation)
    - https://hackage.haskell.org/package/distributed-process-0.6.1
  + Cloud Haskell and Tutorials
    - http://haskell-distributed.github.io/


